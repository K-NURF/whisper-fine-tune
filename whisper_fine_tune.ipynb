{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791a9491",
   "metadata": {},
   "source": [
    "Whisper Fine-tuning Quick Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963239ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU available: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876fa832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Label Studio JSONL export...\n",
      "Loaded 73 samples from Label Studio\n",
      "\n",
      "Dataset overview:\n",
      "                         audio_path  \\\n",
      "0  audio/chunk_3489d71581224dfa.wav   \n",
      "1  audio/chunk_8e7e6b99099516b0.wav   \n",
      "2  audio/chunk_106196e04d812c87.wav   \n",
      "3  audio/chunk_f48d4ca17284401d.wav   \n",
      "4  audio/chunk_f20fbdcc562302c1.wav   \n",
      "\n",
      "                                       transcription   duration  \n",
      "0  Nami buridi. Najichanganya taki bali kwamba la...  12.277875  \n",
      "1  Na... Naona kama vile juhudi zangu zinagonga m...  12.490875  \n",
      "2  Unaeza pia mkajaribu kuongea na mama, sawa. Bi...  11.961875  \n",
      "3  Aaaaah, huko kwote sipo. Mimi ukinipigia 116 u...  12.418875  \n",
      "4  Ni mtendaji wa kijiji. Okay, ni mtendaji wa ki...  11.711875  \n",
      "\n",
      "Transcription length stats (characters):\n",
      "count     73.000000\n",
      "mean     175.424658\n",
      "std       36.083975\n",
      "min       48.000000\n",
      "25%      154.000000\n",
      "50%      177.000000\n",
      "75%      195.000000\n",
      "max      266.000000\n",
      "Name: transcription_length, dtype: float64\n",
      "\n",
      "Duration stats (seconds):\n",
      "count    73.000000\n",
      "mean     11.739594\n",
      "std       1.311267\n",
      "min       4.028000\n",
      "25%      11.901875\n",
      "50%      11.937000\n",
      "75%      12.105875\n",
      "max      13.479000\n",
      "Name: duration, dtype: float64\n",
      "\n",
      "Sample transcriptions:\n",
      "Sample 1: Nami buridi. Najichanganya taki bali kwamba labda imepita ata wiki moja. Kumbe unakuta bado lile yai...\n",
      "Duration: 12.277875 seconds\n",
      "\n",
      "Sample 2: Na... Naona kama vile juhudi zangu zinagonga mwamba, kwa sababu wao naona njia walioichagua ni hiyo ...\n",
      "Duration: 12.490875 seconds\n",
      "\n",
      "Sample 3: Unaeza pia mkajaribu kuongea na mama, sawa. Binti, binti alivyo, wakati anatolewa hapo mliagwa nyiny...\n",
      "Duration: 11.961875 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_labelstudio_export(export_path):\n",
    "    \"\"\"\n",
    "    Load Label Studio JSONL export and extract audio-text pairs\n",
    "    \n",
    "    Expected format per line:\n",
    "    {\"audio_filepath\": \"audio/chunk_xxx.wav\", \"duration\": 12.277875, \"text\": \"transcription\"}\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    with open(export_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                item = json.loads(line.strip())\n",
    "                \n",
    "                # Extract required fields\n",
    "                audio_path = item.get('audio_filepath')\n",
    "                transcription = item.get('text')\n",
    "                duration = item.get('duration')\n",
    "                \n",
    "                # Validate required fields\n",
    "                if audio_path and transcription:\n",
    "                    transcription = transcription.strip()\n",
    "                    \n",
    "                    # Skip very short or empty transcriptions\n",
    "                    if len(transcription) < 3:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip very long transcriptions (might be errors)\n",
    "                    if len(transcription) > 1000:\n",
    "                        continue\n",
    "                    \n",
    "                    processed_data.append({\n",
    "                        'audio_path': audio_path,\n",
    "                        'transcription': transcription,\n",
    "                        'duration': duration\n",
    "                    })\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Load your Label Studio export\n",
    "LABELSTUDIO_EXPORT_PATH = \"label-studio-export/project-11-at-2025-07-08-08-18-a315a9a2/manifest.jsonl\"  # UPDATE THIS PATH (.jsonl extension)\n",
    "AUDIO_BASE_PATH = \"label-studio-export/project-11-at-2025-07-08-08-18-a315a9a2/\"  # UPDATE THIS PATH (should contain the 'audio/' folder)\n",
    "\n",
    "print(\"Loading Label Studio JSONL export...\")\n",
    "labelstudio_data = load_labelstudio_export(LABELSTUDIO_EXPORT_PATH)\n",
    "print(f\"Loaded {len(labelstudio_data)} samples from Label Studio\")\n",
    "\n",
    "# Quick data inspection\n",
    "df = pd.DataFrame(labelstudio_data)\n",
    "print(\"\\nDataset overview:\")\n",
    "print(df.head())\n",
    "print(f\"\\nTranscription length stats (characters):\")\n",
    "df['transcription_length'] = df['transcription'].str.len()\n",
    "print(df['transcription_length'].describe())\n",
    "\n",
    "print(f\"\\nDuration stats (seconds):\")\n",
    "if 'duration' in df.columns:\n",
    "    print(df['duration'].describe())\n",
    "\n",
    "# Sample transcriptions\n",
    "print(f\"\\nSample transcriptions:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"Sample {i+1}: {df.iloc[i]['transcription'][:100]}...\")\n",
    "    print(f\"Duration: {df.iloc[i].get('duration', 'Unknown')} seconds\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b90c4",
   "metadata": {},
   "source": [
    "AUDIO PROCESSING AND QUALITY FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbeeaff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing preprocessed audio files...\n",
      "Base audio path: label-studio-export/project-11-at-2025-07-08-08-18-a315a9a2/\n",
      "Audio files should already be:\n",
      "  - 16kHz sample rate\n",
      "  - Silence removed\n",
      "  - Quality filtered\n",
      "\n",
      "Sample 1:\n",
      "  Original path: audio/chunk_3489d71581224dfa.wav\n",
      "  Full path: label-studio-export/project-11-at-2025-07-08-08-18-a315a9a2/audio/chunk_3489d71581224dfa.wav\n",
      "  File exists: True\n",
      "  Status: SUCCESS - Duration: 12.28s\n",
      "Processed 0/73 samples, 1 valid\n",
      "\n",
      "Sample 2:\n",
      "  Original path: audio/chunk_8e7e6b99099516b0.wav\n",
      "  Full path: label-studio-export/project-11-at-2025-07-08-08-18-a315a9a2/audio/chunk_8e7e6b99099516b0.wav\n",
      "  File exists: True\n",
      "  Status: SUCCESS - Duration: 12.49s\n",
      "\n",
      "Sample 3:\n",
      "  Original path: audio/chunk_106196e04d812c87.wav\n",
      "  Full path: label-studio-export/project-11-at-2025-07-08-08-18-a315a9a2/audio/chunk_106196e04d812c87.wav\n",
      "  File exists: True\n",
      "  Status: SUCCESS - Duration: 11.96s\n",
      "Processed 20/73 samples, 21 valid\n",
      "Processed 40/73 samples, 41 valid\n",
      "Processed 60/73 samples, 61 valid\n",
      "\n",
      "Final dataset: 73 valid samples\n",
      "All samples loaded successfully!\n",
      "\n",
      "Audio duration stats:\n",
      "  Mean: 11.74s\n",
      "  Min: 4.03s\n",
      "  Max: 13.48s\n",
      "  Total: 14.3 minutes\n"
     ]
    }
   ],
   "source": [
    "def load_and_validate_audio(audio_path, target_sr=16000):\n",
    "    \"\"\"Load preprocessed audio file (already cleaned and normalized)\"\"\"\n",
    "    try:\n",
    "        # Handle relative paths from Label Studio\n",
    "        if not os.path.isabs(audio_path):\n",
    "            audio_path = os.path.join(AUDIO_BASE_PATH, audio_path)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(audio_path):\n",
    "            return None, None, f\"File not found: {audio_path}\"\n",
    "        \n",
    "        # Check file size\n",
    "        file_size = os.path.getsize(audio_path)\n",
    "        if file_size == 0:\n",
    "            return None, None, f\"Empty file: {audio_path}\"\n",
    "        \n",
    "        # Load audio (already preprocessed, so minimal validation needed)\n",
    "        audio, sr = sf.read(audio_path)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = np.mean(audio, axis=1)\n",
    "        \n",
    "        # Audio should already be 16kHz, but check\n",
    "        if sr != target_sr:\n",
    "            return None, None, f\"Unexpected sample rate {sr}Hz (expected {target_sr}Hz): {audio_path}\"\n",
    "        \n",
    "        # Check if audio was loaded\n",
    "        if audio is None or len(audio) == 0:\n",
    "            return None, None, f\"Empty audio data: {audio_path}\"\n",
    "        \n",
    "        # Simple duration check (already preprocessed, so should be reasonable)\n",
    "        duration = len(audio) / sr\n",
    "        if duration < 0.1 or duration > 60:  # Very permissive since it's preprocessed\n",
    "            return None, None, f\"Unexpected duration ({duration:.2f}s): {audio_path}\"\n",
    "        \n",
    "        return audio, sr, \"OK\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None, None, f\"Error loading {audio_path}: {str(e)}\"\n",
    "\n",
    "# Process and filter audio files\n",
    "print(\"\\nProcessing preprocessed audio files...\")\n",
    "print(f\"Base audio path: {AUDIO_BASE_PATH}\")\n",
    "print(f\"Audio files should already be:\")\n",
    "print(f\"  - 16kHz sample rate\")\n",
    "print(f\"  - Silence removed\") \n",
    "print(f\"  - Quality filtered\")\n",
    "\n",
    "processed_samples = []\n",
    "failed_samples = []\n",
    "\n",
    "for i, sample in enumerate(labelstudio_data):\n",
    "    audio_path = sample['audio_path']\n",
    "    \n",
    "    # Show first few attempts in detail\n",
    "    if i < 3:\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  Original path: {audio_path}\")\n",
    "        if not os.path.isabs(audio_path):\n",
    "            full_path = os.path.join(AUDIO_BASE_PATH, audio_path)\n",
    "            print(f\"  Full path: {full_path}\")\n",
    "            print(f\"  File exists: {os.path.exists(full_path)}\")\n",
    "        else:\n",
    "            print(f\"  File exists: {os.path.exists(audio_path)}\")\n",
    "    \n",
    "    audio, sr, status = load_and_validate_audio(audio_path)\n",
    "    \n",
    "    if audio is not None:\n",
    "        processed_samples.append({\n",
    "            'audio': audio,\n",
    "            'transcription': sample['transcription'],\n",
    "            'audio_path': sample['audio_path'],\n",
    "            'duration': len(audio) / sr\n",
    "        })\n",
    "        if i < 3:\n",
    "            print(f\"  Status: SUCCESS - Duration: {len(audio)/sr:.2f}s\")\n",
    "    else:\n",
    "        failed_samples.append({'path': audio_path, 'reason': status})\n",
    "        if i < 3:\n",
    "            print(f\"  Status: FAILED - {status}\")\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processed {i}/{len(labelstudio_data)} samples, {len(processed_samples)} valid\")\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(processed_samples)} valid samples\")\n",
    "if failed_samples:\n",
    "    print(f\"Failed samples: {len(failed_samples)}\")\n",
    "    \n",
    "    # Show common failure reasons\n",
    "    print(f\"\\nFailure analysis:\")\n",
    "    failure_reasons = {}\n",
    "    for fail in failed_samples:\n",
    "        reason = fail['reason'].split(':')[0]  # Get main reason\n",
    "        failure_reasons[reason] = failure_reasons.get(reason, 0) + 1\n",
    "    \n",
    "    for reason, count in failure_reasons.items():\n",
    "        print(f\"  {reason}: {count} files\")\n",
    "else:\n",
    "    print(\"All samples loaded successfully!\")\n",
    "\n",
    "# Quick stats on loaded audio\n",
    "if processed_samples:\n",
    "    durations = [s['duration'] for s in processed_samples]\n",
    "    print(f\"\\nAudio duration stats:\")\n",
    "    print(f\"  Mean: {np.mean(durations):.2f}s\")\n",
    "    print(f\"  Min: {np.min(durations):.2f}s\") \n",
    "    print(f\"  Max: {np.max(durations):.2f}s\")\n",
    "    print(f\"  Total: {np.sum(durations)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ea53c",
   "metadata": {},
   "source": [
    "PREPARE DATASET FOR WHISPER TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdfb8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded openai/whisper-small\n",
      "\n",
      "Preparing dataset for training...\n",
      "Training samples: 58\n",
      "Validation samples: 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Whisper processor and model\n",
    "model_name = \"openai/whisper-small\"  # Start with small for quick experiments\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "print(f\"Loaded {model_name}\")\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator for speech-to-text models - Fixed version from working code\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # Pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset(samples, processor, test_size=0.2):\n",
    "    \"\"\"Convert processed samples to Hugging Face dataset format\"\"\"\n",
    "    \n",
    "    def process_sample(sample):\n",
    "        # Process audio to log-mel spectrogram\n",
    "        input_features = processor.feature_extractor(\n",
    "            sample['audio'], \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features[0]\n",
    "        \n",
    "        # Tokenize transcription\n",
    "        labels = processor.tokenizer(\n",
    "            sample['transcription'], \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=448\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "    \n",
    "    # Process all samples\n",
    "    processed = [process_sample(sample) for sample in samples]\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_list(processed)\n",
    "    \n",
    "    # Train/test split\n",
    "    dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Prepare dataset\n",
    "print(\"\\nPreparing dataset for training...\")\n",
    "dataset = prepare_dataset(processed_samples, processor)\n",
    "\n",
    "print(f\"Training samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e90af",
   "metadata": {},
   "source": [
    "EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3748ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute WER metric during training\"\"\"\n",
    "    pred_ids = eval_pred.predictions\n",
    "    label_ids = eval_pred.label_ids\n",
    "\n",
    "    # Replace -100 with pad token id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e059c54",
   "metadata": {},
   "source": [
    "TRAINING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration set for Whisper training\n",
      "Training arguments configured with proper gradient checkpointing\n",
      "Initializing trainer...\n",
      "Trainer initialized successfully with evaluation enabled\n"
     ]
    }
   ],
   "source": [
    "# Data collator - Standard configuration\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL CONFIGURATION - STANDARD WHISPER SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Standard Whisper model configuration\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "print(\"Model configuration set for Whisper training\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING ARGUMENTS - BASELINE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Standard training arguments for RTX 4060 Ti (16GB VRAM)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-finetuned-experiment\",\n",
    "    \n",
    "    # Batch sizes optimized for 16GB VRAM\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,      # Effective batch size = 4*4 = 16\n",
    "    \n",
    "    # Learning configuration\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=1000,\n",
    "    \n",
    "    # Memory and performance settings\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "    fp16=True,\n",
    "    \n",
    "    # Evaluation settings\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    \n",
    "    # Saving and logging\n",
    "    save_steps=100,\n",
    "    logging_steps=25,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Integration\n",
    "    report_to=[\"mlflow\"],\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured with standard settings\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINER INITIALIZATION \n",
    "# =============================================================================\n",
    "\n",
    "print(\"Initializing trainer...\")\n",
    "\n",
    "# Initialize trainer with standard configuration\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3cabdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BASELINE EVALUATION\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline WER: 2.8528\n",
      "\n",
      "Baseline Examples:\n",
      "\n",
      "Example 1:\n",
      "Reference: Mwanangu kwa sababu Mungu alijaliwa alikuwa kashapona. Nikaona hayana haja, nikayaacha. Sasa nimekaa juzi natumiwa tena mtoto, tena sio mtoto wangu, w...\n",
      "Prediction:  naka...\n",
      "\n",
      "Example 2:\n",
      "Reference: Ndoa za utotoni, ni pale ambapo binti anabeba mimba akiwa chini ya miaka kumi na nane....\n",
      "Prediction:  Doza ututu ni pari ya npako binti ana beba mi ba kiyo, siri wa mi ya kakumina nane....\n",
      "\n",
      "Example 3:\n",
      "Reference: Uhhhh. Mmmmh. Badala ya kumpa maziwa ya wanyama....\n",
      "Prediction:  Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa....\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BASELINE EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_baseline(test_samples, model, processor):\n",
    "    \"\"\"Evaluate baseline Whisper model\"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in test_samples[:10]:\n",
    "            # Process audio\n",
    "            input_features = processor(\n",
    "                sample['audio'], \n",
    "                sampling_rate=16000, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features.to(device)\n",
    "            \n",
    "            # Generate transcription\n",
    "            predicted_ids = model.generate(input_features)[0]\n",
    "            prediction = processor.decode(predicted_ids, skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            references.append(sample['transcription'])\n",
    "    \n",
    "    # Calculate WER\n",
    "    baseline_wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    print(f\"Baseline WER: {baseline_wer:.4f}\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\nBaseline Examples:\")\n",
    "    for i in range(min(3, len(predictions))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Reference: {references[i][:100]}...\")\n",
    "        print(f\"Prediction: {predictions[i][:100]}...\")\n",
    "    \n",
    "    return baseline_wer, predictions, references\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_wer, baseline_preds, baseline_refs = evaluate_baseline(\n",
    "    processed_samples[-20:], model, processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ac656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: http://localhost:5000\n",
      "MLflow experiment: <Experiment: artifact_location='/mlflow/artifacts/8', creation_time=1752065869356, experiment_id='8', last_update_time=1752065869356, lifecycle_stage='active', name='whisper-quick-experiment-2', tags={}>\n",
      "\n",
      "==================================================\n",
      "BASELINE EVALUATION\n",
      "==================================================\n",
      "Baseline WER: 2.8528\n",
      "\n",
      "Baseline Examples:\n",
      "\n",
      "Example 1:\n",
      "Reference: Mwanangu kwa sababu Mungu alijaliwa alikuwa kashapona. Nikaona hayana haja, nikayaacha. Sasa nimekaa juzi natumiwa tena mtoto, tena sio mtoto wangu, w...\n",
      "Prediction:  naka...\n",
      "\n",
      "Example 2:\n",
      "Reference: Ndoa za utotoni, ni pale ambapo binti anabeba mimba akiwa chini ya miaka kumi na nane....\n",
      "Prediction:  Doza ututu ni pari ya npako binti ana beba mi ba kiyo, siri wa mi ya kakumina nane....\n",
      "\n",
      "Example 3:\n",
      "Reference: Uhhhh. Mmmmh. Badala ya kumpa maziwa ya wanyama....\n",
      "Prediction:  Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa....\n",
      "\n",
      "==================================================\n",
      "STARTING TRAINING\n",
      "==================================================\n",
      "Active run detected: d4a46886b38940aa8df81a35cba71d58\n",
      "üèÉ View run ./whisper-finetuned-quick-test at: http://localhost:5000/#/experiments/8/runs/d4a46886b38940aa8df81a35cba71d58\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/8\n",
      "MLflow logging started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 34:08, Epoch 250/250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.120100</td>\n",
       "      <td>2.860643</td>\n",
       "      <td>1.668269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>2.754059</td>\n",
       "      <td>1.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>3.130589</td>\n",
       "      <td>1.360577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>3.240578</td>\n",
       "      <td>1.728365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>3.312905</td>\n",
       "      <td>1.733173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.359595</td>\n",
       "      <td>1.733173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.387628</td>\n",
       "      <td>1.348558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.404866</td>\n",
       "      <td>1.334135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.414341</td>\n",
       "      <td>1.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.418957</td>\n",
       "      <td>1.334135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: WER = 1.67%\n",
      "Evaluation at step 100: {'eval_loss': 2.860643148422241, 'eval_wer': 1.6682692307692308}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franklin/.virtualenvs/model_training/lib/python3.12/site-packages/transformers/modeling_utils.py:3685: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: WER = 1.77%\n",
      "Evaluation at step 200: {'eval_loss': 2.754058837890625, 'eval_wer': 1.7692307692307692}\n",
      "Step 300: WER = 1.36%\n",
      "Evaluation at step 300: {'eval_loss': 3.1305885314941406, 'eval_wer': 1.3605769230769231}\n",
      "Step 400: WER = 1.73%\n",
      "Evaluation at step 400: {'eval_loss': 3.2405784130096436, 'eval_wer': 1.7283653846153846}\n",
      "Step 500: WER = 1.73%\n",
      "Evaluation at step 500: {'eval_loss': 3.3129053115844727, 'eval_wer': 1.7331730769230769}\n",
      "Step 600: WER = 1.73%\n",
      "Evaluation at step 600: {'eval_loss': 3.3595945835113525, 'eval_wer': 1.7331730769230769}\n",
      "Step 700: WER = 1.35%\n",
      "Evaluation at step 700: {'eval_loss': 3.387627601623535, 'eval_wer': 1.3485576923076923}\n",
      "Step 800: WER = 1.33%\n",
      "Evaluation at step 800: {'eval_loss': 3.4048657417297363, 'eval_wer': 1.3341346153846154}\n",
      "Step 900: WER = 1.35%\n",
      "Evaluation at step 900: {'eval_loss': 3.4143412113189697, 'eval_wer': 1.3461538461538463}\n",
      "Step 1000: WER = 1.33%\n",
      "Evaluation at step 1000: {'eval_loss': 3.418957233428955, 'eval_wer': 1.3341346153846154}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Loss = 0.4374\n",
      "üèÉ View run whisper-small-quick-test at: http://localhost:5000/#/experiments/8/runs/1ccc5f4f71974256a644dd0fe7e8bdff\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/8\n",
      "Training completed! MLflow run ready for final evaluation.\n",
      "\n",
      "==================================================\n",
      "FINAL EVALUATION\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: WER = 1.33%\n",
      "Evaluation at step 1000: {'eval_loss': 3.4048657417297363, 'eval_wer': 1.3341346153846154, 'eval_runtime': 20.7909, 'eval_samples_per_second': 0.721, 'eval_steps_per_second': 0.721, 'epoch': 250.0}\n",
      "Final WER: 1.33%\n",
      "Baseline WER: 285.28%\n",
      "WER Improvement: 99.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/10 12:52:28 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Cannot pickle a prepared model with automatic mixed precision, please unwrap the model with `Accelerator.unwrap_model(model)` before pickling it.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 176\u001b[39m\n\u001b[32m    173\u001b[39m     trainer.save_model()\n\u001b[32m    175\u001b[39m     \u001b[38;5;66;03m# Log model to MLflow\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpytorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhisper-finetuned-quick\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel saved to MLflow!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# End MLflow run safely\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/model_training/lib/python3.12/site-packages/mlflow/pytorch/__init__.py:288\u001b[39m, in \u001b[36mlog_model\u001b[39m\u001b[34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, extra_files, pip_requirements, extra_pip_requirements, metadata, name, params, tags, model_type, step, model_id, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03mLog a PyTorch model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    285\u001b[39m \u001b[33;03m    PyTorch logged models\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m pickle_module = pickle_module \u001b[38;5;129;01mor\u001b[39;00m mlflow_pytorch_pickle_module\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpytorch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconda_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconda_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/model_training/lib/python3.12/site-packages/mlflow/models/model.py:1207\u001b[39m, in \u001b[36mModel.log\u001b[39m\u001b[34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001b[39m\n\u001b[32m   1195\u001b[39m     prompts = [pr.uri \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pr, PromptVersion) \u001b[38;5;28;01melse\u001b[39;00m pr \u001b[38;5;28;01mfor\u001b[39;00m pr \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m   1197\u001b[39m mlflow_model = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m   1198\u001b[39m     artifact_path=model.artifact_location,\n\u001b[32m   1199\u001b[39m     model_uuid=model.model_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1205\u001b[39m     model_id=model.model_id,\n\u001b[32m   1206\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1207\u001b[39m \u001b[43mflavor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlflow_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[38;5;66;03m# `save_model` calls `load_model` to infer the model requirements, which may result\u001b[39;00m\n\u001b[32m   1209\u001b[39m \u001b[38;5;66;03m# in __pycache__ directories being created in the model directory.\u001b[39;00m\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pycache \u001b[38;5;129;01min\u001b[39;00m Path(local_path).rglob(\u001b[33m\"\u001b[39m\u001b[33m__pycache__\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/model_training/lib/python3.12/site-packages/mlflow/pytorch/__init__.py:461\u001b[39m, in \u001b[36msave_model\u001b[39m\u001b[34m(pytorch_model, path, conda_env, mlflow_model, code_paths, pickle_module, signature, input_example, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[39m\n\u001b[32m    459\u001b[39m     torch.jit.ScriptModule.save(pytorch_model, model_path)\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m torchserve_artifacts_config = {}\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extra_files:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/model_training/lib/python3.12/site-packages/torch/serialization.py:965\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m    964\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/model_training/lib/python3.12/site-packages/torch/serialization.py:1211\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1208\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m persistent_id(obj)\n\u001b[32m   1210\u001b[39m pickler = PyTorchPickler(data_buf, protocol=pickle_protocol)\n\u001b[32m-> \u001b[39m\u001b[32m1211\u001b[39m \u001b[43mpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1212\u001b[39m data_value = data_buf.getvalue()\n\u001b[32m   1213\u001b[39m zip_file.write_record(\u001b[33m\"\u001b[39m\u001b[33mdata.pkl\u001b[39m\u001b[33m\"\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/model_training/lib/python3.12/site-packages/cloudpickle/cloudpickle.py:1303\u001b[39m, in \u001b[36mPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m   1301\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[32m   1302\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1303\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1304\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1305\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e.args) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrecursion\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e.args[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/model_training/lib/python3.12/site-packages/accelerate/utils/operations.py:809\u001b[39m, in \u001b[36mConvertOutputsToFp32.__getstate__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(\n\u001b[32m    810\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot pickle a prepared model with automatic mixed precision, please unwrap the model with `Accelerator.unwrap_model(model)` before pickling it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    811\u001b[39m     )\n",
      "\u001b[31mPicklingError\u001b[39m: Cannot pickle a prepared model with automatic mixed precision, please unwrap the model with `Accelerator.unwrap_model(model)` before pickling it."
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "# Clear any existing MLflow runs\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# MLflow Configuration\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Update to your MLflow server URI\n",
    "mlflow.set_experiment(\"whisper-quick-experiment-2\")\n",
    "\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow experiment: {mlflow.get_experiment_by_name('whisper-quick-experiment-2')}\")\n",
    "\n",
    "class MLflowCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback for real-time MLflow logging\"\"\"\n",
    "    \n",
    "    def __init__(self, run_name=\"whisper-small-quick-test\"):\n",
    "        self.run_name = run_name\n",
    "        self.mlflow_run = None\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Start MLflow run at beginning of training\"\"\"\n",
    "        # Check for active run and end it if necessary\n",
    "        if mlflow.active_run():\n",
    "            print(f\"Active run detected: {mlflow.active_run().info.run_id}\")\n",
    "            mlflow.end_run()\n",
    "        \n",
    "        # Start new MLflow run\n",
    "        self.mlflow_run = mlflow.start_run(run_name=self.run_name)\n",
    "        \n",
    "        # Log training configuration\n",
    "        mlflow.log_param(\"model_name\", \"openai/whisper-small\")\n",
    "        mlflow.log_param(\"dataset_size\", len(processed_samples))\n",
    "        mlflow.log_param(\"train_size\", len(dataset[\"train\"]))\n",
    "        mlflow.log_param(\"val_size\", len(dataset[\"test\"]))\n",
    "        mlflow.log_param(\"learning_rate\", args.learning_rate)\n",
    "        mlflow.log_param(\"batch_size\", args.per_device_train_batch_size)\n",
    "        mlflow.log_param(\"effective_batch_size\", args.per_device_train_batch_size * args.gradient_accumulation_steps)\n",
    "        mlflow.log_param(\"max_steps\", args.max_steps)\n",
    "        mlflow.log_param(\"fp16\", args.fp16)\n",
    "        mlflow.log_param(\"gradient_checkpointing\", args.gradient_checkpointing)\n",
    "        mlflow.log_param(\"eval_steps\", args.eval_steps)\n",
    "        mlflow.log_param(\"warmup_steps\", args.warmup_steps)\n",
    "        \n",
    "        print(\"MLflow logging started!\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Log metrics in real-time during training\"\"\"\n",
    "        if logs and self.mlflow_run:\n",
    "            step = state.global_step\n",
    "            \n",
    "            # Log all metrics from the logs\n",
    "            for key, value in logs.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    mlflow.log_metric(key, value, step=step)\n",
    "            \n",
    "            # Log epoch\n",
    "            mlflow.log_metric(\"epoch\", state.epoch, step=step)\n",
    "            \n",
    "            # Print key metrics for monitoring\n",
    "            if \"train_loss\" in logs:\n",
    "                print(f\"Step {step}: Loss = {logs['train_loss']:.4f}\")\n",
    "            if \"eval_wer\" in logs:\n",
    "                print(f\"Step {step}: WER = {logs['eval_wer']:.2f}%\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"Log evaluation metrics\"\"\"\n",
    "        if metrics and self.mlflow_run:\n",
    "            step = state.global_step\n",
    "            \n",
    "            for key, value in metrics.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    mlflow.log_metric(key, value, step=step)\n",
    "            \n",
    "            print(f\"Evaluation at step {step}: {metrics}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Log final training completion\"\"\"\n",
    "        if self.mlflow_run:\n",
    "            print(\"Training completed! MLflow run ready for final evaluation.\")\n",
    "\n",
    "# Initialize MLflow callback\n",
    "mlflow_callback = MLflowCallback()\n",
    "\n",
    "# Add callback to trainer\n",
    "trainer.add_callback(mlflow_callback)\n",
    "\n",
    "# =============================================================================\n",
    "# BASELINE EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_baseline(test_samples, model, processor):\n",
    "    \"\"\"Evaluate baseline Whisper model - simplified approach\"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in test_samples[:10]:  # Evaluate on first 10 samples for speed\n",
    "            # Process audio - simplified approach like reference code\n",
    "            input_features = processor(\n",
    "                sample['audio'], \n",
    "                sampling_rate=16000, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features.to(device)\n",
    "            \n",
    "            # Generate transcription - simplified like reference code\n",
    "            predicted_ids = model.generate(input_features)[0]\n",
    "            prediction = processor.decode(predicted_ids, skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            references.append(sample['transcription'])\n",
    "    \n",
    "    # Calculate WER\n",
    "    baseline_wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    print(f\"Baseline WER: {baseline_wer:.4f}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\nBaseline Examples:\")\n",
    "    for i in range(min(3, len(predictions))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Reference: {references[i][:150]}...\")  # Truncate for readability\n",
    "        print(f\"Prediction: {predictions[i][:150]}...\")\n",
    "    \n",
    "    return baseline_wer, predictions, references\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_wer, baseline_preds, baseline_refs = evaluate_baseline(\n",
    "    processed_samples[-20:], model, processor\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING EXECUTION WITH MLflow TRACKING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Training will automatically start MLflow run and log metrics\n",
    "trainer.train()\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL EVALUATION AND MODEL SAVING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_eval = trainer.evaluate()\n",
    "final_wer = final_eval['eval_wer']\n",
    "\n",
    "print(f\"Final WER: {final_wer:.2f}%\")\n",
    "print(f\"Baseline WER: {baseline_wer*100:.2f}%\")\n",
    "improvement = ((baseline_wer*100 - final_wer) / (baseline_wer*100)) * 100\n",
    "print(f\"WER Improvement: {improvement:.2f}%\")\n",
    "\n",
    "# Log final metrics to MLflow (ensure we're in the run context)\n",
    "if mlflow.active_run():\n",
    "    mlflow.log_metric(\"final_wer\", final_wer)\n",
    "    mlflow.log_metric(\"baseline_wer\", baseline_wer * 100)  # Convert to percentage\n",
    "    mlflow.log_metric(\"wer_improvement_percent\", improvement)\n",
    "\n",
    "    # Save model\n",
    "    trainer.save_model()\n",
    "\n",
    "    # Log model to MLflow\n",
    "    try:\n",
    "        # Method 1: Use trainer's unwrapped model\n",
    "        unwrapped_model = trainer.accelerator.unwrap_model(trainer.model)\n",
    "        \n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=unwrapped_model,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=\"whisper-finetuned-quick\"\n",
    "        )\n",
    "        print(\"Model saved to MLflow!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MLflow model logging failed: {e}\")\n",
    "        print(\"Model was saved locally with trainer.save_model()\")\n",
    "        \n",
    "        # Alternative: Log just the model state dict\n",
    "        try:\n",
    "            model_state = {\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'model_config': trainer.model.config,\n",
    "            }\n",
    "            mlflow.pytorch.log_state_dict(model_state, artifact_path=\"model_state\")\n",
    "            print(\"Model state dict saved to MLflow as fallback\")\n",
    "        except:\n",
    "            print(\"All MLflow model logging attempts failed - model saved locally only\")\n",
    "\n",
    "# End MLflow run safely\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline WER: {baseline_wer*100:.2f}%\")\n",
    "print(f\"Fine-tuned WER: {final_wer:.2f}%\")\n",
    "print(f\"Improvement: {improvement:.2f}%\")\n",
    "print(\"\\nView results in MLflow UI:\")\n",
    "print(\"Run: mlflow ui\")\n",
    "print(\"Open: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6547bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING FINE-TUNED MODEL\n",
      "==================================================\n",
      "\n",
      "Test Sample 1:\n",
      "Reference:  Ni maeneo gani? Je, anaishi wapi huyo mtu? Ni maeneo , ni moja wa mwanafamilia, kama ni mwanafamilia, je ana mahusiana yapi, ni baba, ni mama?...\n",
      "Prediction:  Anahishi wapi huyo mtu. Ni maeneo, ni moja wa mwanafamilia, kama ni mwanafamilia je ana mahusiana yapi ni baba, ni mama...\n",
      "\n",
      "Test Sample 2:\n",
      "Reference:  Democracy. Katika democracy tukasoma moja, mbili, tatu. Sasa unaeza kumbuka, aaaaha, mwalimu hapa alivyoelezea, akatoa mfano hivi. Kuwa unaeza kukumbu...\n",
      "Prediction:  demokrasi, katika demokrasi utukasuma moja mbilitato kwafana kukumboka hapa mwalimu ya walezea, akatowa mfano hivi, kwao kukumboka piawa kupitia mfan...\n",
      "\n",
      "Test Sample 3:\n",
      "Reference:  Ambaye mtoto huyo ataelekezwa kuenda kukaa sawa. Eeeeh? Eeeeh. Mtoto ana haki ya kulindwa dhidi ya ukatili wa aina yoyote ile. Iwe ukatili wa kimwili,...\n",
      "Prediction:  kwa mbaye mtotowe ataelekeza kuenda kukaa sawa. Mtoto ana haki ya kulindwa dhidi ya ukatili wa aina yoyote ile. Iwe ukatili wa kimwili, kihisia, king...\n",
      "\n",
      "Test Sample 4:\n",
      "Reference:  Kwa kuwa umri huu mtoto anakuwa anakula vitu vitu. Mara aokote hiki aeke mdomoni, inaeza ikasababisha kwa kiasi fulani kupata minyoo. Na hii minyoo pi...\n",
      "Prediction:  Kwa kuwa umri huu mtoto anakuwa anakula vitu vitu, mara wakote hiki aeke mdomoni, inaeza ikasababisha kwa kiasi fulani kupata minyio. Na hii minyio p...\n",
      "\n",
      "Test Sample 5:\n",
      "Reference:  Huenda ikatokea aina zingine za manyanyaso either katika ngali ya familia ama ya mtaani au shuleni ambazo zinaleta athari katika nini? Katika hali aki...\n",
      "Prediction:  katokea maina zingine za manyanyaso, hiza katika ngali ya familia ama ya mtaani au shuleni ambazo zinaleta athari katika nini? Katika haki ya mtu. Ya...\n",
      "\n",
      "Test WER on 5 samples: 0.4196\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS:\n",
      "============================================================\n",
      "1. If results look good, increase max_steps for longer training\n",
      "2. Experiment with different learning rates and batch sizes\n",
      "3. Try larger Whisper models (medium, large)\n",
      "4. Implement the full MLOps structure we discussed\n",
      "5. Add more sophisticated data preprocessing\n",
      "6. Implement cross-validation and more robust evaluation\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def test_finetuned_model(test_samples, trainer, processor, num_samples=5):\n",
    "    \"\"\"Test the fine-tuned model on new samples with improved generation\"\"\"\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(test_samples[-num_samples:]):  # Test on last few samples\n",
    "            # Process audio\n",
    "            input_features = processor.feature_extractor(\n",
    "                sample['audio'], \n",
    "                sampling_rate=16000, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features.to(device)\n",
    "            \n",
    "            # Generate transcription with improved parameters\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                max_length=448,  # Increased max length\n",
    "                num_beams=5,     # Beam search for better quality\n",
    "                early_stopping=True,\n",
    "                do_sample=False,  # Deterministic generation\n",
    "                pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"sw\", task=\"transcribe\")\n",
    "            )\n",
    "            prediction = processor.tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            references.append(sample['transcription'])\n",
    "            \n",
    "            print(f\"\\nTest Sample {i+1}:\")\n",
    "            print(f\"Reference:  {sample['transcription'][:150]}...\")  # Truncate for readability\n",
    "            print(f\"Prediction: {prediction[:150]}...\")\n",
    "    \n",
    "    # Calculate final WER on test samples\n",
    "    test_wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"\\nTest WER on {num_samples} samples: {test_wer:.4f}\")\n",
    "    \n",
    "    return test_wer\n",
    "\n",
    "# Test fine-tuned model\n",
    "test_wer = test_finetuned_model(processed_samples, trainer, processor)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. If results look good, increase max_steps for longer training\")\n",
    "print(\"2. Experiment with different learning rates and batch sizes\")\n",
    "print(\"3. Try larger Whisper models (medium, large)\")\n",
    "print(\"4. Implement the full MLOps structure we discussed\")\n",
    "print(\"5. Add more sophisticated data preprocessing\")\n",
    "print(\"6. Implement cross-validation and more robust evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
