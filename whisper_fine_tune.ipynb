{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791a9491",
   "metadata": {},
   "source": [
    "Whisper Fine-tuning Quick Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963239ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU available: True\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876fa832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Label Studio JSONL export...\n",
      "Loaded 174 samples from Label Studio\n",
      "\n",
      "Dataset overview:\n",
      "                              audio_path  \\\n",
      "0       audio/chunk_3489d71581224dfa.wav   \n",
      "1  audio/chunk_3489d71581224dfa_a64e.wav   \n",
      "2       audio/chunk_8e7e6b99099516b0.wav   \n",
      "3       audio/chunk_106196e04d812c87.wav   \n",
      "4       audio/chunk_f48d4ca17284401d.wav   \n",
      "\n",
      "                                       transcription   duration  \n",
      "0  Nami buridi. Najichanganya taki bali kwamba la...  12.277875  \n",
      "1  Nami buridi. Najichanganya taki bali kwamba la...  12.277875  \n",
      "2  Na... Naona kama vile juhudi zangu zinagonga m...  12.490875  \n",
      "3  Unaeza pia mkajaribu kuongea na mama, sawa. Bi...  11.961875  \n",
      "4  Aaaaah, huko kwote sipo. Mimi ukinipigia 116 u...  12.418875  \n",
      "\n",
      "Transcription length stats (characters):\n",
      "count    174.000000\n",
      "mean     179.103448\n",
      "std       36.861100\n",
      "min       45.000000\n",
      "25%      160.000000\n",
      "50%      179.500000\n",
      "75%      196.750000\n",
      "max      278.000000\n",
      "Name: transcription_length, dtype: float64\n",
      "\n",
      "Duration stats (seconds):\n",
      "count    174.000000\n",
      "mean      11.775328\n",
      "std        1.402447\n",
      "min        3.451875\n",
      "25%       11.912750\n",
      "50%       11.989937\n",
      "75%       12.105875\n",
      "max       13.479000\n",
      "Name: duration, dtype: float64\n",
      "\n",
      "Sample transcriptions:\n",
      "Sample 1: Nami buridi. Najichanganya taki bali kwamba labda imepita ata wiki moja. Kumbe unakuta bado lile yai...\n",
      "Duration: 12.277875 seconds\n",
      "\n",
      "Sample 2: Nami buridi. Najichanganya taki bali kwamba labda imepita ata wiki moja. Kumbe unakuta bado lile yai...\n",
      "Duration: 12.277875 seconds\n",
      "\n",
      "Sample 3: Na... Naona kama vile juhudi zangu zinagonga mwamba, kwa sababu wao naona njia walioichagua ni hiyo ...\n",
      "Duration: 12.490875 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_labelstudio_export(export_path):\n",
    "    \"\"\"\n",
    "    Load Label Studio JSONL export and extract audio-text pairs\n",
    "    \n",
    "    Expected format per line:\n",
    "    {\"audio_filepath\": \"audio/chunk_xxx.wav\", \"duration\": 12.277875, \"text\": \"transcription\"}\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    with open(export_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                item = json.loads(line.strip())\n",
    "                \n",
    "                # Extract required fields\n",
    "                audio_path = item.get('audio_filepath')\n",
    "                transcription = item.get('text')\n",
    "                duration = item.get('duration')\n",
    "                \n",
    "                # Validate required fields\n",
    "                if audio_path and transcription:\n",
    "                    transcription = transcription.strip()\n",
    "                    \n",
    "                    # Skip very short or empty transcriptions\n",
    "                    if len(transcription) < 3:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip very long transcriptions (might be errors)\n",
    "                    if len(transcription) > 1000:\n",
    "                        continue\n",
    "                    \n",
    "                    processed_data.append({\n",
    "                        'audio_path': audio_path,\n",
    "                        'transcription': transcription,\n",
    "                        'duration': duration\n",
    "                    })\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Load your Label Studio export\n",
    "LABELSTUDIO_EXPORT_PATH = \"label-studio-export/data-v1/manifest.jsonl\"  # UPDATE THIS PATH (.jsonl extension)\n",
    "AUDIO_BASE_PATH = \"label-studio-export/data-v1\"  # UPDATE THIS PATH (should contain the 'audio/' folder)\n",
    "\n",
    "print(\"Loading Label Studio JSONL export...\")\n",
    "labelstudio_data = load_labelstudio_export(LABELSTUDIO_EXPORT_PATH)\n",
    "print(f\"Loaded {len(labelstudio_data)} samples from Label Studio\")\n",
    "\n",
    "# Quick data inspection\n",
    "df = pd.DataFrame(labelstudio_data)\n",
    "print(\"\\nDataset overview:\")\n",
    "print(df.head())\n",
    "print(f\"\\nTranscription length stats (characters):\")\n",
    "df['transcription_length'] = df['transcription'].str.len()\n",
    "print(df['transcription_length'].describe())\n",
    "\n",
    "print(f\"\\nDuration stats (seconds):\")\n",
    "if 'duration' in df.columns:\n",
    "    print(df['duration'].describe())\n",
    "\n",
    "# Sample transcriptions\n",
    "print(f\"\\nSample transcriptions:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"Sample {i+1}: {df.iloc[i]['transcription'][:100]}...\")\n",
    "    print(f\"Duration: {df.iloc[i].get('duration', 'Unknown')} seconds\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b90c4",
   "metadata": {},
   "source": [
    "AUDIO PROCESSING AND QUALITY FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbeeaff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing preprocessed audio files...\n",
      "Base audio path: label-studio-export/data-v1\n",
      "Audio files should already be:\n",
      "  - 16kHz sample rate\n",
      "  - Silence removed\n",
      "  - Quality filtered\n",
      "\n",
      "Sample 1:\n",
      "  Original path: audio/chunk_3489d71581224dfa.wav\n",
      "  Full path: label-studio-export/data-v1/audio/chunk_3489d71581224dfa.wav\n",
      "  File exists: True\n",
      "  Status: SUCCESS - Duration: 12.28s\n",
      "Processed 0/174 samples, 1 valid\n",
      "\n",
      "Sample 2:\n",
      "  Original path: audio/chunk_3489d71581224dfa_a64e.wav\n",
      "  Full path: label-studio-export/data-v1/audio/chunk_3489d71581224dfa_a64e.wav\n",
      "  File exists: True\n",
      "  Status: SUCCESS - Duration: 12.28s\n",
      "\n",
      "Sample 3:\n",
      "  Original path: audio/chunk_8e7e6b99099516b0.wav\n",
      "  Full path: label-studio-export/data-v1/audio/chunk_8e7e6b99099516b0.wav\n",
      "  File exists: True\n",
      "  Status: SUCCESS - Duration: 12.49s\n",
      "Processed 20/174 samples, 21 valid\n",
      "Processed 40/174 samples, 41 valid\n",
      "Processed 60/174 samples, 61 valid\n",
      "Processed 80/174 samples, 81 valid\n",
      "Processed 100/174 samples, 101 valid\n",
      "Processed 120/174 samples, 121 valid\n",
      "Processed 140/174 samples, 141 valid\n",
      "Processed 160/174 samples, 161 valid\n",
      "\n",
      "Final dataset: 174 valid samples\n",
      "All samples loaded successfully!\n",
      "\n",
      "Audio duration stats:\n",
      "  Mean: 11.78s\n",
      "  Min: 3.45s\n",
      "  Max: 13.48s\n",
      "  Total: 34.1 minutes\n"
     ]
    }
   ],
   "source": [
    "def load_and_validate_audio(audio_path, target_sr=16000):\n",
    "    \"\"\"Load preprocessed audio file (already cleaned and normalized)\"\"\"\n",
    "    try:\n",
    "        # Handle relative paths from Label Studio\n",
    "        if not os.path.isabs(audio_path):\n",
    "            audio_path = os.path.join(AUDIO_BASE_PATH, audio_path)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(audio_path):\n",
    "            return None, None, f\"File not found: {audio_path}\"\n",
    "        \n",
    "        # Check file size\n",
    "        file_size = os.path.getsize(audio_path)\n",
    "        if file_size == 0:\n",
    "            return None, None, f\"Empty file: {audio_path}\"\n",
    "        \n",
    "        # Load audio (already preprocessed, so minimal validation needed)\n",
    "        audio, sr = sf.read(audio_path)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = np.mean(audio, axis=1)\n",
    "        \n",
    "        # Audio should already be 16kHz, but check\n",
    "        if sr != target_sr:\n",
    "            return None, None, f\"Unexpected sample rate {sr}Hz (expected {target_sr}Hz): {audio_path}\"\n",
    "        \n",
    "        # Check if audio was loaded\n",
    "        if audio is None or len(audio) == 0:\n",
    "            return None, None, f\"Empty audio data: {audio_path}\"\n",
    "        \n",
    "        # Simple duration check (already preprocessed, so should be reasonable)\n",
    "        duration = len(audio) / sr\n",
    "        if duration < 0.1 or duration > 60:  # Very permissive since it's preprocessed\n",
    "            return None, None, f\"Unexpected duration ({duration:.2f}s): {audio_path}\"\n",
    "        \n",
    "        return audio, sr, \"OK\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None, None, f\"Error loading {audio_path}: {str(e)}\"\n",
    "\n",
    "# Process and filter audio files\n",
    "print(\"\\nProcessing preprocessed audio files...\")\n",
    "print(f\"Base audio path: {AUDIO_BASE_PATH}\")\n",
    "print(f\"Audio files should already be:\")\n",
    "print(f\"  - 16kHz sample rate\")\n",
    "print(f\"  - Silence removed\") \n",
    "print(f\"  - Quality filtered\")\n",
    "\n",
    "processed_samples = []\n",
    "failed_samples = []\n",
    "\n",
    "for i, sample in enumerate(labelstudio_data):\n",
    "    audio_path = sample['audio_path']\n",
    "    \n",
    "    # Show first few attempts in detail\n",
    "    if i < 3:\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  Original path: {audio_path}\")\n",
    "        if not os.path.isabs(audio_path):\n",
    "            full_path = os.path.join(AUDIO_BASE_PATH, audio_path)\n",
    "            print(f\"  Full path: {full_path}\")\n",
    "            print(f\"  File exists: {os.path.exists(full_path)}\")\n",
    "        else:\n",
    "            print(f\"  File exists: {os.path.exists(audio_path)}\")\n",
    "    \n",
    "    audio, sr, status = load_and_validate_audio(audio_path)\n",
    "    \n",
    "    if audio is not None:\n",
    "        processed_samples.append({\n",
    "            'audio': audio,\n",
    "            'transcription': sample['transcription'],\n",
    "            'audio_path': sample['audio_path'],\n",
    "            'duration': len(audio) / sr\n",
    "        })\n",
    "        if i < 3:\n",
    "            print(f\"  Status: SUCCESS - Duration: {len(audio)/sr:.2f}s\")\n",
    "    else:\n",
    "        failed_samples.append({'path': audio_path, 'reason': status})\n",
    "        if i < 3:\n",
    "            print(f\"  Status: FAILED - {status}\")\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processed {i}/{len(labelstudio_data)} samples, {len(processed_samples)} valid\")\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(processed_samples)} valid samples\")\n",
    "if failed_samples:\n",
    "    print(f\"Failed samples: {len(failed_samples)}\")\n",
    "    \n",
    "    # Show common failure reasons\n",
    "    print(f\"\\nFailure analysis:\")\n",
    "    failure_reasons = {}\n",
    "    for fail in failed_samples:\n",
    "        reason = fail['reason'].split(':')[0]  # Get main reason\n",
    "        failure_reasons[reason] = failure_reasons.get(reason, 0) + 1\n",
    "    \n",
    "    for reason, count in failure_reasons.items():\n",
    "        print(f\"  {reason}: {count} files\")\n",
    "else:\n",
    "    print(\"All samples loaded successfully!\")\n",
    "\n",
    "# Quick stats on loaded audio\n",
    "if processed_samples:\n",
    "    durations = [s['duration'] for s in processed_samples]\n",
    "    print(f\"\\nAudio duration stats:\")\n",
    "    print(f\"  Mean: {np.mean(durations):.2f}s\")\n",
    "    print(f\"  Min: {np.min(durations):.2f}s\") \n",
    "    print(f\"  Max: {np.max(durations):.2f}s\")\n",
    "    print(f\"  Total: {np.sum(durations)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ea53c",
   "metadata": {},
   "source": [
    "PREPARE DATASET FOR WHISPER TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdfb8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded openai/whisper-small\n",
      "\n",
      "Preparing dataset for training...\n",
      "Training samples: 139\n",
      "Validation samples: 35\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Whisper processor and model\n",
    "model_name = \"openai/whisper-small\"  # Start with small for quick experiments\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "print(f\"Loaded {model_name}\")\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator for speech-to-text models \n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # Pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset(samples, processor, test_size=0.2):\n",
    "    \"\"\"Convert processed samples to Hugging Face dataset format\"\"\"\n",
    "    \n",
    "    def process_sample(sample):\n",
    "        # Process audio to log-mel spectrogram\n",
    "        input_features = processor.feature_extractor(\n",
    "            sample['audio'], \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features[0]\n",
    "        \n",
    "        # Tokenize transcription\n",
    "        labels = processor.tokenizer(\n",
    "            sample['transcription'], \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=448\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "    \n",
    "    # Process all samples\n",
    "    processed = [process_sample(sample) for sample in samples]\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_list(processed)\n",
    "    \n",
    "    # Train/test split\n",
    "    dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Prepare dataset\n",
    "print(\"\\nPreparing dataset for training...\")\n",
    "dataset = prepare_dataset(processed_samples, processor)\n",
    "\n",
    "print(f\"Training samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e90af",
   "metadata": {},
   "source": [
    "EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3748ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute WER metric during training\"\"\"\n",
    "    pred_ids = eval_pred.predictions\n",
    "    label_ids = eval_pred.label_ids\n",
    "\n",
    "    # Replace -100 with pad token id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e059c54",
   "metadata": {},
   "source": [
    "TRAINING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d5ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration set for Whisper training\n",
      "Training arguments configured with standard settings\n",
      "Initializing trainer...\n",
      "Trainer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Data collator - Standard configuration\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL CONFIGURATION - STANDARD WHISPER SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Standard Whisper model configuration\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "print(\"Model configuration set for Whisper training\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING ARGUMENTS - BASELINE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Standard training arguments for RTX 4060 Ti (16GB VRAM)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-finetuned-experiment\",\n",
    "    \n",
    "    # Batch sizes optimized for 16GB VRAM\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,      # Effective batch size = 4*4 = 16\n",
    "    \n",
    "    # Learning configuration\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=1000,\n",
    "    \n",
    "    # Memory and performance settings\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "    fp16=True,\n",
    "    \n",
    "    # Evaluation settings\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    \n",
    "    # Saving and logging\n",
    "    save_steps=100,\n",
    "    logging_steps=25,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Integration\n",
    "    report_to=[\"mlflow\"],\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured with standard settings\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINER INITIALIZATION \n",
    "# =============================================================================\n",
    "\n",
    "print(\"Initializing trainer...\")\n",
    "\n",
    "# Initialize trainer with standard configuration\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3cabdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BASELINE EVALUATION\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline WER: 2.7563\n",
      "\n",
      "Baseline Examples:\n",
      "\n",
      "Example 1:\n",
      "Reference: Yaani hajiami kiu kweli yaani hata ukimtuma kitu mmh yaani hana uhakika anachokifanya kaa sijajua Ch...\n",
      "Prediction:  kwa...\n",
      "\n",
      "Example 2:\n",
      "Reference: Uhhhh. Mmmmh. Badala ya kumpa maziwa ya wanyama....\n",
      "Prediction:  Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwa...\n",
      "\n",
      "Example 3:\n",
      "Reference: Mtoto anatakiwa awaskilize. Mkimwite baba yenu, mkitaka kukaa naye na kueleza na nyinyi matatizo yen...\n",
      "Prediction:  kwa...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BASELINE EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_baseline(test_samples, model, processor):\n",
    "    \"\"\"Evaluate baseline Whisper model\"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in test_samples[:10]:\n",
    "            # Process audio\n",
    "            input_features = processor(\n",
    "                sample['audio'], \n",
    "                sampling_rate=16000, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features.to(device)\n",
    "            \n",
    "            # Generate transcription\n",
    "            predicted_ids = model.generate(input_features)[0]\n",
    "            prediction = processor.decode(predicted_ids, skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            references.append(sample['transcription'])\n",
    "    \n",
    "    # Calculate WER\n",
    "    baseline_wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    print(f\"Baseline WER: {baseline_wer:.4f}\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\nBaseline Examples:\")\n",
    "    for i in range(min(3, len(predictions))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Reference: {references[i][:100]}...\")\n",
    "        print(f\"Prediction: {predictions[i][:100]}...\")\n",
    "    \n",
    "    return baseline_wer, predictions, references\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_wer, baseline_preds, baseline_refs = evaluate_baseline(\n",
    "    processed_samples[-20:], model, processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120ac656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: http://localhost:5000\n",
      "MLflow experiment: <Experiment: artifact_location='mlflow-artifacts:/317544992576262060', creation_time=1752185332532, experiment_id='317544992576262060', last_update_time=1752185332532, lifecycle_stage='active', name='whisper-quick-experiment-2', tags={}>\n",
      "\n",
      "==================================================\n",
      "BASELINE EVALUATION\n",
      "==================================================\n",
      "Baseline WER: 2.7563\n",
      "\n",
      "Baseline Examples:\n",
      "\n",
      "Example 1:\n",
      "Reference: Yaani hajiami kiu kweli yaani hata ukimtuma kitu mmh yaani hana uhakika anachokifanya kaa sijajua Changamoto hapa ni ipi ambayo  nilikuwa nakutana nay...\n",
      "Prediction:  kwa...\n",
      "\n",
      "Example 2:\n",
      "Reference: Uhhhh. Mmmmh. Badala ya kumpa maziwa ya wanyama....\n",
      "Prediction:  Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa. Mwaa....\n",
      "\n",
      "Example 3:\n",
      "Reference: Mtoto anatakiwa awaskilize. Mkimwite baba yenu, mkitaka kukaa naye na kueleza na nyinyi matatizo yenu, au changamoto zenu mlizo nazo, anatakiwa akae a...\n",
      "Prediction:  kwa...\n",
      "\n",
      "==================================================\n",
      "STARTING TRAINING\n",
      "==================================================\n",
      "Active run detected: 292aaa3c660c4f34bc347c855fa71f14\n",
      "ðŸƒ View run ./whisper-finetuned-experiment at: http://localhost:5000/#/experiments/317544992576262060/runs/292aaa3c660c4f34bc347c855fa71f14\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/317544992576262060\n",
      "MLflow logging started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 148\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Training will automatically start MLflow run and log metrics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# FINAL EVALUATION AND MODEL SAVING\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/transformers/trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/transformers/trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/transformers/trainer.py:3797\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3795\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3797\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3799\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2549\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2548\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/torch/autograd/function.py:307\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    305\u001b[39m     )\n\u001b[32m    306\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:320\u001b[39m, in \u001b[36mCheckpointFunction.backward\u001b[39m\u001b[34m(ctx, *args)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) == \u001b[32m0\u001b[39m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    317\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnone of output has requires_grad=True,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    318\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m this checkpoint() is not necessary\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    319\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m grads = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    322\u001b[39m     inp.grad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs\n\u001b[32m    324\u001b[39m )\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) + grads\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper-fine-tune/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "# Clear any existing MLflow runs\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# MLflow Configuration\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Update to your MLflow server URI\n",
    "mlflow.set_experiment(\"whisper-quick-experiment-2\")\n",
    "\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow experiment: {mlflow.get_experiment_by_name('whisper-quick-experiment-2')}\")\n",
    "\n",
    "class MLflowCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback for real-time MLflow logging\"\"\"\n",
    "    \n",
    "    def __init__(self, run_name=\"whisper-small-quick-test\"):\n",
    "        self.run_name = run_name\n",
    "        self.mlflow_run = None\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Start MLflow run at beginning of training\"\"\"\n",
    "        # Check for active run and end it if necessary\n",
    "        if mlflow.active_run():\n",
    "            print(f\"Active run detected: {mlflow.active_run().info.run_id}\")\n",
    "            mlflow.end_run()\n",
    "        \n",
    "        # Start new MLflow run\n",
    "        self.mlflow_run = mlflow.start_run(run_name=self.run_name)\n",
    "        \n",
    "        # Log training configuration\n",
    "        mlflow.log_param(\"model_name\", \"openai/whisper-small\")\n",
    "        mlflow.log_param(\"dataset_size\", len(processed_samples))\n",
    "        mlflow.log_param(\"train_size\", len(dataset[\"train\"]))\n",
    "        mlflow.log_param(\"val_size\", len(dataset[\"test\"]))\n",
    "        mlflow.log_param(\"learning_rate\", args.learning_rate)\n",
    "        mlflow.log_param(\"batch_size\", args.per_device_train_batch_size)\n",
    "        mlflow.log_param(\"effective_batch_size\", args.per_device_train_batch_size * args.gradient_accumulation_steps)\n",
    "        mlflow.log_param(\"max_steps\", args.max_steps)\n",
    "        mlflow.log_param(\"fp16\", args.fp16)\n",
    "        mlflow.log_param(\"gradient_checkpointing\", args.gradient_checkpointing)\n",
    "        mlflow.log_param(\"eval_steps\", args.eval_steps)\n",
    "        mlflow.log_param(\"warmup_steps\", args.warmup_steps)\n",
    "        \n",
    "        print(\"MLflow logging started!\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Log metrics in real-time during training\"\"\"\n",
    "        if logs and self.mlflow_run:\n",
    "            step = state.global_step\n",
    "            \n",
    "            # Log all metrics from the logs\n",
    "            for key, value in logs.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    mlflow.log_metric(key, value, step=step)\n",
    "            \n",
    "            # Log epoch\n",
    "            mlflow.log_metric(\"epoch\", state.epoch, step=step)\n",
    "            \n",
    "            # Print key metrics for monitoring\n",
    "            if \"train_loss\" in logs:\n",
    "                print(f\"Step {step}: Loss = {logs['train_loss']:.4f}\")\n",
    "            if \"eval_wer\" in logs:\n",
    "                print(f\"Step {step}: WER = {logs['eval_wer']:.2f}%\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"Log evaluation metrics\"\"\"\n",
    "        if metrics and self.mlflow_run:\n",
    "            step = state.global_step\n",
    "            \n",
    "            for key, value in metrics.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    mlflow.log_metric(key, value, step=step)\n",
    "            \n",
    "            print(f\"Evaluation at step {step}: {metrics}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Log final training completion\"\"\"\n",
    "        if self.mlflow_run:\n",
    "            print(\"Training completed! MLflow run ready for final evaluation.\")\n",
    "\n",
    "# Initialize MLflow callback\n",
    "mlflow_callback = MLflowCallback()\n",
    "\n",
    "# Add callback to trainer\n",
    "trainer.add_callback(mlflow_callback)\n",
    "\n",
    "# =============================================================================\n",
    "# BASELINE EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_baseline(test_samples, model, processor):\n",
    "    \"\"\"Evaluate baseline Whisper model - simplified approach\"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in test_samples[:10]:  # Evaluate on first 10 samples for speed\n",
    "            # Process audio - simplified approach like reference code\n",
    "            input_features = processor(\n",
    "                sample['audio'], \n",
    "                sampling_rate=16000, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features.to(device)\n",
    "            \n",
    "            # Generate transcription - simplified like reference code\n",
    "            predicted_ids = model.generate(input_features)[0]\n",
    "            prediction = processor.decode(predicted_ids, skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            references.append(sample['transcription'])\n",
    "    \n",
    "    # Calculate WER\n",
    "    baseline_wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    print(f\"Baseline WER: {baseline_wer:.4f}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\nBaseline Examples:\")\n",
    "    for i in range(min(3, len(predictions))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Reference: {references[i][:150]}...\")  # Truncate for readability\n",
    "        print(f\"Prediction: {predictions[i][:150]}...\")\n",
    "    \n",
    "    return baseline_wer, predictions, references\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_wer, baseline_preds, baseline_refs = evaluate_baseline(\n",
    "    processed_samples[-20:], model, processor\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING EXECUTION WITH MLflow TRACKING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Training will automatically start MLflow run and log metrics\n",
    "trainer.train()\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL EVALUATION AND MODEL SAVING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_eval = trainer.evaluate()\n",
    "final_wer = final_eval['eval_wer']\n",
    "\n",
    "print(f\"Final WER: {final_wer:.2f}%\")\n",
    "print(f\"Baseline WER: {baseline_wer*100:.2f}%\")\n",
    "improvement = ((baseline_wer*100 - final_wer) / (baseline_wer*100)) * 100\n",
    "print(f\"WER Improvement: {improvement:.2f}%\")\n",
    "\n",
    "# Log final metrics to MLflow (ensure we're in the run context)\n",
    "if mlflow.active_run():\n",
    "    mlflow.log_metric(\"final_wer\", final_wer)\n",
    "    mlflow.log_metric(\"baseline_wer\", baseline_wer * 100)  # Convert to percentage\n",
    "    mlflow.log_metric(\"wer_improvement_percent\", improvement)\n",
    "\n",
    "    # Save model\n",
    "    trainer.save_model()\n",
    "\n",
    "    # Log model to MLflow\n",
    "    try:\n",
    "        # Method 1: Use trainer's unwrapped model\n",
    "        unwrapped_model = trainer.accelerator.unwrap_model(trainer.model)\n",
    "        \n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=unwrapped_model,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=\"whisper-finetuned-quick\"\n",
    "        )\n",
    "        print(\"Model saved to MLflow!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MLflow model logging failed: {e}\")\n",
    "        print(\"Model was saved locally with trainer.save_model()\")\n",
    "        \n",
    "        # Alternative: Log just the model state dict\n",
    "        try:\n",
    "            model_state = {\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'model_config': trainer.model.config,\n",
    "            }\n",
    "            mlflow.pytorch.log_state_dict(model_state, artifact_path=\"model_state\")\n",
    "            print(\"Model state dict saved to MLflow as fallback\")\n",
    "        except:\n",
    "            print(\"All MLflow model logging attempts failed - model saved locally only\")\n",
    "\n",
    "# End MLflow run safely\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline WER: {baseline_wer*100:.2f}%\")\n",
    "print(f\"Fine-tuned WER: {final_wer:.2f}%\")\n",
    "print(f\"Improvement: {improvement:.2f}%\")\n",
    "print(\"\\nView results in MLflow UI:\")\n",
    "print(\"Run: mlflow ui\")\n",
    "print(\"Open: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6547bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING FINE-TUNED MODEL\n",
      "==================================================\n",
      "\n",
      "Test Sample 1:\n",
      "Reference:  Ni maeneo gani? Je, anaishi wapi huyo mtu? Ni maeneo , ni moja wa mwanafamilia, kama ni mwanafamilia, je ana mahusiana yapi, ni baba, ni mama?...\n",
      "Prediction:  Anahishi wapi huyo mtu. Ni maeneo, ni moja wa mwanafamilia, kama ni mwanafamilia je ana mahusiana yapi ni baba, ni mama...\n",
      "\n",
      "Test Sample 2:\n",
      "Reference:  Democracy. Katika democracy tukasoma moja, mbili, tatu. Sasa unaeza kumbuka, aaaaha, mwalimu hapa alivyoelezea, akatoa mfano hivi. Kuwa unaeza kukumbu...\n",
      "Prediction:  demokrasi, katika demokrasi utukasuma moja mbilitato kwafana kukumboka hapa mwalimu ya walezea, akatowa mfano hivi, kwao kukumboka piawa kupitia mfan...\n",
      "\n",
      "Test Sample 3:\n",
      "Reference:  Ambaye mtoto huyo ataelekezwa kuenda kukaa sawa. Eeeeh? Eeeeh. Mtoto ana haki ya kulindwa dhidi ya ukatili wa aina yoyote ile. Iwe ukatili wa kimwili,...\n",
      "Prediction:  kwa mbaye mtotowe ataelekeza kuenda kukaa sawa. Mtoto ana haki ya kulindwa dhidi ya ukatili wa aina yoyote ile. Iwe ukatili wa kimwili, kihisia, king...\n",
      "\n",
      "Test Sample 4:\n",
      "Reference:  Kwa kuwa umri huu mtoto anakuwa anakula vitu vitu. Mara aokote hiki aeke mdomoni, inaeza ikasababisha kwa kiasi fulani kupata minyoo. Na hii minyoo pi...\n",
      "Prediction:  Kwa kuwa umri huu mtoto anakuwa anakula vitu vitu, mara wakote hiki aeke mdomoni, inaeza ikasababisha kwa kiasi fulani kupata minyio. Na hii minyio p...\n",
      "\n",
      "Test Sample 5:\n",
      "Reference:  Huenda ikatokea aina zingine za manyanyaso either katika ngali ya familia ama ya mtaani au shuleni ambazo zinaleta athari katika nini? Katika hali aki...\n",
      "Prediction:  katokea maina zingine za manyanyaso, hiza katika ngali ya familia ama ya mtaani au shuleni ambazo zinaleta athari katika nini? Katika haki ya mtu. Ya...\n",
      "\n",
      "Test WER on 5 samples: 0.4196\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS:\n",
      "============================================================\n",
      "1. If results look good, increase max_steps for longer training\n",
      "2. Experiment with different learning rates and batch sizes\n",
      "3. Try larger Whisper models (medium, large)\n",
      "4. Implement the full MLOps structure we discussed\n",
      "5. Add more sophisticated data preprocessing\n",
      "6. Implement cross-validation and more robust evaluation\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def test_finetuned_model(test_samples, trainer, processor, num_samples=5):\n",
    "    \"\"\"Test the fine-tuned model on new samples with improved generation\"\"\"\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(test_samples[-num_samples:]):  # Test on last few samples\n",
    "            # Process audio\n",
    "            input_features = processor.feature_extractor(\n",
    "                sample['audio'], \n",
    "                sampling_rate=16000, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features.to(device)\n",
    "            \n",
    "            # Generate transcription with improved parameters\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                max_length=448,  # Increased max length\n",
    "                num_beams=5,     # Beam search for better quality\n",
    "                early_stopping=True,\n",
    "                do_sample=False,  # Deterministic generation\n",
    "                pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"sw\", task=\"transcribe\")\n",
    "            )\n",
    "            prediction = processor.tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            references.append(sample['transcription'])\n",
    "            \n",
    "            print(f\"\\nTest Sample {i+1}:\")\n",
    "            print(f\"Reference:  {sample['transcription'][:150]}...\")  # Truncate for readability\n",
    "            print(f\"Prediction: {prediction[:150]}...\")\n",
    "    \n",
    "    # Calculate final WER on test samples\n",
    "    test_wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"\\nTest WER on {num_samples} samples: {test_wer:.4f}\")\n",
    "    \n",
    "    return test_wer\n",
    "\n",
    "# Test fine-tuned model\n",
    "test_wer = test_finetuned_model(processed_samples, trainer, processor)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. If results look good, increase max_steps for longer training\")\n",
    "print(\"2. Experiment with different learning rates and batch sizes\")\n",
    "print(\"3. Try larger Whisper models (medium, large)\")\n",
    "print(\"4. Implement the full MLOps structure we discussed\")\n",
    "print(\"5. Add more sophisticated data preprocessing\")\n",
    "print(\"6. Implement cross-validation and more robust evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
